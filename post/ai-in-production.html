<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <meta name="description" content="What breaks when you ship AI in production" />
  <meta name="author" content="Akarsh Cholapurath" />
  <title>What breaks when you ship AI in production</title>

  <style>
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap');

    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }

    body {
      background: #ffffff;
      color: #000000;
      font-family: 'Inter', -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica Neue', Arial, sans-serif;
      font-size: 16px;
      line-height: 1.6;
      padding: 20px;
    }

    h1, h2 {
      font-weight: normal;
      margin-bottom: 0.75em;
    }

    h1 {
      font-size: 26px;
      margin-bottom: 1em;
    }

    h2 {
      font-size: 20px;
      border-bottom: 2px dashed #000;
      padding-bottom: 4px;
      margin-top: 2.2em;
    }

    p {
      margin-bottom: 1.5em;
    }

    ul {
      margin-left: 28px;
      margin-bottom: 1.5em;
    }

    li {
      margin-bottom: 0.75em;
    }

    pre {
      background: #f4f4f4;
      border: 1px solid #000;
      padding: 14px;
      margin: 1.5em 0;
      overflow-x: auto;
      font-size: 14px;
      line-height: 1.45;
    }

    code {
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, monospace;
    }

    .container {
      max-width: 768px;
      margin: 0 auto;
      padding: 8px;
    }

    .meta {
      font-size: 13px;
      color: #666;
      margin-bottom: 1.5em;
    }
    .back-link {
      color: #000000;
      text-decoration: underline;
      font-size: 16px;
      margin-bottom: 1.5em;
      display: inline-block;
    }
    .back-link:hover {
      color: #000000;
      background: #c0c0c0;
    }
    .back-link:visited {
      color: #000000;
    }
  </style>
</head>

<body>
  <div class="container">
    <a href="../blog.html" class="back-link">&larr; Back to blog</a>
    <h1>What breaks when you ship AI in production</h1>
    <p class="meta">January 28, 2026 · by Akarsh</p>

    <p>
      Shipping AI in production is very different from experimenting with AI in a demo or side project.
    </p>

    <p>
      I have worked on and shipped multiple AI systems that reached real users and real traffic. The hard part was never calling an AI API and getting a response back. That part is easy. The real challenges start when AI becomes a <strong>core part of your production workflow</strong>.
    </p>

    <p>
      Once AI requests become large, slow, and asynchronous, you stop building features and start running a distributed system.
    </p>

    <h2>Failure modes I did not expect</h2>

    <p>
      The first production AI system I shipped did not fail loudly. It failed quietly.
    </p>

    <pre><code>User clicks "Generate"
→ API enqueues background job
→ API returns 200 OK
</code></pre>

    <p>
      Everything meaningful happened after the HTTP request had already completed.
    </p>

    <p>
      In one case, a background job crashed after a partial model response. No exception bubbled up. No retry triggered. The user waited indefinitely for a WebSocket message that never arrived.
    </p>

    <p>
      In another case, a model returned valid JSON that passed schema validation but was semantically wrong. Downstream code treated it as success, and corrupted state propagated silently.
    </p>

    <p>
      Schema validation can ensure structure, but it cannot tell you whether the content is meaningfully correct. That distinction only became obvious after things broke.
    </p>

    <p>
      These failures were not rare. They happened multiple times per day once traffic increased.
    </p>

    <h2>Why async AI failures go silent</h2>

    <p>
      Most AI integrations are written as if the system is synchronous, even when it is not.
    </p>

    <pre><code>POST /generate
  enqueue(job)
  return 200
</code></pre>

    <p>
      Everything important happens after the response is sent. If a job stalls, times out, or crashes, the user-facing request has no way to reflect that unless state is explicitly tracked.
    </p>

    <p>
      Retries often made this worse. A retry might eventually succeed, masking the original failure and making root-cause analysis difficult.
    </p>

    <h2>Before and after: making async state explicit</h2>

    <p>
      One of the biggest improvements came from making async state transitions explicit instead of implicit.
    </p>

    <pre><code>// BEFORE: implicit state
async function generate(jobId, input) {
  const result = await callAIModel(input)
  await saveResult(jobId, result)
  await notifyClient(jobId, result)
}
</code></pre>

    <pre><code>// AFTER: explicit state tracking
async function generate(jobId, input) {
  updateState(jobId, "started")

  try {
    updateState(jobId, "model_call_started")
    const result = await callAIModel(input)

    updateState(jobId, "model_call_succeeded")
    await saveResult(jobId, result)

    updateState(jobId, "completed")
    await notifyClient(jobId, result)
  } catch (err) {
    updateState(jobId, "failed", { error: err.message })
    throw err
  }
}
</code></pre>

    <p>
      Once state transitions were persisted, failures became boring. It was always clear where a request stopped progressing.
    </p>

    <h2>Debugging techniques that actually helped</h2>

    <ul>
      <li>
        Treat every async step as a state machine and persist transitions.
      </li>
      <li>
        Log raw request and response payloads before any retry logic runs.
      </li>
      <li>
        Track queue and execution timing explicitly. In one system, roughly 70% of perceived model slowness turned out to be job backlog and scheduling delay.
      </li>
      <li>
        Track missing events explicitly. If a job never emits a completion event within an expected window, that is a failure signal.
      </li>
    </ul>

    <h2>Integration testing was burning money</h2>

    <p>
      During early development, most failures had nothing to do with model quality. Prompts changed. Response structures evolved. Parsers broke. State handling was wrong.
    </p>

    <p>
      Yet every failed attempt still resulted in a real API call and real cost.
    </p>

    <p>
      The breakthrough was separating workflow correctness from model correctness.
    </p>

    <pre><code>// Deterministic response for integration testing
return {
  status: "completed",
  output: {
    title: "Sample title",
    summary: "Sample summary"
  }
}
</code></pre>

    <p>
      This made it possible to validate background jobs, retries, WebSocket delivery, and state transitions without introducing cost or non-determinism.
    </p>

    <h2>Local development was lying to me</h2>

    <p>
      Locally, jobs ran instantly. WebSockets never disconnected. Timeouts never triggered. None of the real failure modes appeared.
    </p>

    <p>
      The biggest improvement came from being able to observe async execution locally in real time.
    </p>

    <p>
      Forwarding async events, retries, and failures to localhost made production behavior visible early, instead of discovering it after deployment.
    </p>

    <p>
      The experience was similar to the first time I used a webhook CLI and realized how much production behavior I had been blind to.
    </p>

    <h2>What I wish I had known earlier</h2>

    <ul>
      <li>Async AI systems fail by omission, not exception.</li>
      <li>Retries without visibility hide more bugs than they fix.</li>
      <li>Most AI “slowness” is actually queueing and coordination.</li>
      <li>The system around the model matters more than the model itself.</li>
    </ul>

    <p>
      Once I started treating AI workflows as distributed systems first and model calls second, failures became easier to reason about and far less surprising.
    </p>

    <h2>Closing thoughts</h2>

    <p>
      Developers should not have to rediscover these lessons the hard way just to ship a production-grade AI application.
    </p>

    <p>
      I’m currently working on ModelRiver, which grew out of repeatedly hitting these exact problems while shipping AI systems in production.
    </p>

    <p style="margin-top: 2em;">
      Thanks for reading. You can find me on <a href="https://x.com/akarshcp" target="_blank">X</a>.
    </p>
  </div>
</body>
</html>
